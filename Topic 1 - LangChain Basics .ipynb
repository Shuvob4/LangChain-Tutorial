{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2ea25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining openai_api_key\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d13f040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking openai version\n",
    "import openai\n",
    "\n",
    "openai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3972178c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.9'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking langchain version\n",
    "import langchain\n",
    "\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae9c311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openai== (from versions: 0.0.2, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.2.0, 0.2.1, 0.2.3, 0.2.4, 0.2.5, 0.2.6, 0.3.0, 0.4.0, 0.6.0, 0.6.1, 0.6.2, 0.6.3, 0.6.4, 0.7.0, 0.8.0, 0.9.0, 0.9.1, 0.9.2, 0.9.3, 0.9.4, 0.10.0, 0.10.1, 0.10.2, 0.10.3, 0.10.4, 0.10.5, 0.11.0, 0.11.1, 0.11.2, 0.11.3, 0.11.4, 0.11.5, 0.11.6, 0.12.0, 0.13.0, 0.14.0, 0.15.0, 0.16.0, 0.18.0, 0.18.1, 0.19.0, 0.20.0, 0.22.0, 0.22.1, 0.23.0, 0.23.1, 0.24.0, 0.25.0, 0.26.0, 0.26.1, 0.26.2, 0.26.3, 0.26.4, 0.26.5, 0.27.0, 0.27.1, 0.27.2, 0.27.3, 0.27.4, 0.27.5, 0.27.6, 0.27.7, 0.27.8, 0.27.9, 0.27.10, 0.28.0, 0.28.1, 1.0.0b1, 1.0.0b2, 1.0.0b3, 1.0.0rc1, 1.0.0rc2, 1.0.0rc3, 1.0.0, 1.0.1, 1.1.0, 1.1.1, 1.1.2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.8, 1.3.9, 1.4.0, 1.5.0, 1.6.0, 1.6.1, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.9.0, 1.10.0, 1.11.0, 1.11.1, 1.12.0, 1.13.3)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openai==\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# checking the available version of openai\n",
    "%pip install openai=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228e7240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement langchain== (from versions: 0.0.1, 0.0.2, 0.0.3, 0.0.4, 0.0.5, 0.0.6, 0.0.7, 0.0.8, 0.0.9, 0.0.10, 0.0.11, 0.0.12, 0.0.13, 0.0.14, 0.0.15, 0.0.16, 0.0.17, 0.0.18, 0.0.19, 0.0.20, 0.0.21, 0.0.22, 0.0.23, 0.0.24, 0.0.25, 0.0.26, 0.0.27, 0.0.28, 0.0.29, 0.0.30, 0.0.31, 0.0.32, 0.0.33, 0.0.34, 0.0.35, 0.0.36, 0.0.37, 0.0.38, 0.0.39, 0.0.40, 0.0.41, 0.0.42, 0.0.43, 0.0.44, 0.0.45, 0.0.46, 0.0.47, 0.0.48, 0.0.49, 0.0.50, 0.0.51, 0.0.52, 0.0.53, 0.0.54, 0.0.55, 0.0.56, 0.0.57, 0.0.58, 0.0.59, 0.0.60, 0.0.61, 0.0.63, 0.0.64, 0.0.65, 0.0.66, 0.0.67, 0.0.68, 0.0.69, 0.0.70, 0.0.71, 0.0.72, 0.0.73, 0.0.74, 0.0.75, 0.0.76, 0.0.77, 0.0.78, 0.0.79, 0.0.80, 0.0.81, 0.0.82, 0.0.83, 0.0.84, 0.0.85, 0.0.86, 0.0.87, 0.0.88, 0.0.89, 0.0.90, 0.0.91, 0.0.92, 0.0.93, 0.0.94, 0.0.95, 0.0.96, 0.0.97, 0.0.98, 0.0.99rc0, 0.0.99, 0.0.100, 0.0.101rc0, 0.0.101, 0.0.102rc0, 0.0.102, 0.0.103, 0.0.104, 0.0.105, 0.0.106, 0.0.107, 0.0.108, 0.0.109, 0.0.110, 0.0.111, 0.0.112, 0.0.113, 0.0.114, 0.0.115, 0.0.116, 0.0.117, 0.0.118, 0.0.119, 0.0.120, 0.0.121, 0.0.122, 0.0.123, 0.0.124, 0.0.125, 0.0.126, 0.0.127, 0.0.128, 0.0.129, 0.0.130, 0.0.131, 0.0.132, 0.0.133, 0.0.134, 0.0.135, 0.0.136, 0.0.137, 0.0.138, 0.0.139, 0.0.140, 0.0.141, 0.0.142, 0.0.143, 0.0.144, 0.0.145, 0.0.146, 0.0.147, 0.0.148, 0.0.149, 0.0.150, 0.0.151, 0.0.152, 0.0.153, 0.0.154, 0.0.155, 0.0.156, 0.0.157, 0.0.158, 0.0.159, 0.0.160, 0.0.161, 0.0.162, 0.0.163, 0.0.164, 0.0.165, 0.0.166, 0.0.167, 0.0.168, 0.0.169, 0.0.170, 0.0.171, 0.0.172, 0.0.173, 0.0.174, 0.0.175, 0.0.176, 0.0.177, 0.0.178, 0.0.179, 0.0.180, 0.0.181, 0.0.182, 0.0.183, 0.0.184, 0.0.185, 0.0.186, 0.0.187, 0.0.188, 0.0.189, 0.0.190, 0.0.191, 0.0.192, 0.0.193, 0.0.194, 0.0.195, 0.0.196, 0.0.197, 0.0.198, 0.0.199, 0.0.200, 0.0.201, 0.0.202, 0.0.203, 0.0.204, 0.0.205, 0.0.206, 0.0.207, 0.0.208, 0.0.209, 0.0.210, 0.0.211, 0.0.212, 0.0.213, 0.0.214, 0.0.215, 0.0.216, 0.0.217, 0.0.218, 0.0.219, 0.0.220, 0.0.221, 0.0.222, 0.0.223, 0.0.224, 0.0.225, 0.0.226, 0.0.227, 0.0.228, 0.0.229, 0.0.230, 0.0.231, 0.0.232, 0.0.233, 0.0.234, 0.0.235, 0.0.236, 0.0.237, 0.0.238, 0.0.239, 0.0.240rc0, 0.0.240rc1, 0.0.240rc4, 0.0.240, 0.0.242, 0.0.243, 0.0.244, 0.0.245, 0.0.246, 0.0.247, 0.0.248, 0.0.249, 0.0.250, 0.0.251, 0.0.252, 0.0.253, 0.0.254, 0.0.255, 0.0.256, 0.0.257, 0.0.258, 0.0.259, 0.0.260, 0.0.261, 0.0.262, 0.0.263, 0.0.264, 0.0.265, 0.0.266, 0.0.267, 0.0.268, 0.0.269, 0.0.270, 0.0.271, 0.0.272, 0.0.273, 0.0.274, 0.0.275, 0.0.276, 0.0.277, 0.0.278, 0.0.279, 0.0.281, 0.0.283, 0.0.284, 0.0.285, 0.0.286, 0.0.287, 0.0.288, 0.0.289, 0.0.290, 0.0.291, 0.0.292, 0.0.293, 0.0.294, 0.0.295, 0.0.296, 0.0.297, 0.0.298, 0.0.299, 0.0.300, 0.0.301, 0.0.302, 0.0.303, 0.0.304, 0.0.305, 0.0.306, 0.0.307, 0.0.308, 0.0.309, 0.0.310, 0.0.311, 0.0.312, 0.0.313, 0.0.314, 0.0.315, 0.0.316, 0.0.317, 0.0.318, 0.0.319, 0.0.320, 0.0.321, 0.0.322, 0.0.323, 0.0.324, 0.0.325, 0.0.326, 0.0.327, 0.0.329, 0.0.330, 0.0.331rc0, 0.0.331rc1, 0.0.331rc2, 0.0.331rc3, 0.0.331, 0.0.332, 0.0.333, 0.0.334, 0.0.335, 0.0.336, 0.0.337, 0.0.338, 0.0.339rc0, 0.0.339rc1, 0.0.339rc2, 0.0.339rc3, 0.0.339, 0.0.340, 0.0.341, 0.0.342, 0.0.343, 0.0.344, 0.0.345, 0.0.346, 0.0.347, 0.0.348, 0.0.349rc1, 0.0.349rc2, 0.0.349, 0.0.350, 0.0.351, 0.0.352, 0.0.353, 0.0.354, 0.1.0, 0.1.1, 0.1.2, 0.1.3, 0.1.4, 0.1.5, 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.1.10, 0.1.11)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain==\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# checking the available version of langchain\n",
    "%pip install langchain=="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcc92b",
   "metadata": {},
   "source": [
    "## Defining the very first Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "597d840d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuvobarman/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# instantiate the model\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82031bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-openai\n",
      "  Obtaining dependency information for langchain-openai from https://files.pythonhosted.org/packages/81/63/012be16114559243aabcc9ec570366df84591dc9f8f3c2349a398e9b3626/langchain_openai-0.0.8-py3-none-any.whl.metadata\n",
      "  Downloading langchain_openai-0.0.8-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-openai) (0.1.27)\n",
      "Requirement already satisfied: openai<2.0.0,>=1.10.0 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-openai) (1.12.0)\n",
      "Requirement already satisfied: tiktoken<1,>=0.5.2 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-openai) (0.6.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (6.0)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (0.1.10)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.10.8)\n",
      "Requirement already satisfied: requests<3,>=2 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain-openai) (8.2.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (0.27.0)\n",
      "Requirement already satisfied: sniffio in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (1.2.0)\n",
      "Requirement already satisfied: tqdm>4 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from openai<2.0.0,>=1.10.0->langchain-openai) (4.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from tiktoken<1,>=0.5.2->langchain-openai) (2022.7.9)\n",
      "Requirement already satisfied: idna>=2.8 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.4)\n",
      "Requirement already satisfied: certifi in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.27->langchain-openai) (3.9.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain-openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./anaconda3/envs/langchain/lib/python3.11/site-packages (from requests<3,>=2->langchain-core<0.2.0,>=0.1.27->langchain-openai) (1.26.16)\n",
      "Downloading langchain_openai-0.0.8-py3-none-any.whl (32 kB)\n",
      "Installing collected packages: langchain-openai\n",
      "Successfully installed langchain-openai-0.0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install -U langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b63a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32156d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "372f86b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuvobarman/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As an AI, I don't have feelings or emotions, but I'm here to assist you with anything you need. How can I help you today?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.predict('How are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfc0ef0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings or emotions. How can I assist you today?\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('How are you ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655358ac",
   "metadata": {},
   "source": [
    "## LLMs\n",
    "\n",
    "In langchain it's really easy to implement various Large Language Models. By default it uses GPT-3 but we can also define that. Please make sure to look into the pricing for using the higher version of GPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "999ff0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuvobarman/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nI am an AI and do not have the capability to feel emotions. However, I am functioning well and ready to assist you with any tasks you may need help with. Is there something specific you would like me to do?'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm.invoke('How are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7795de19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nI am an AI language model, I do not have the ability to feel emotions. How can I assist you?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "llm.invoke('How are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9b0450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm just a computer program, so I don't have feelings or emotions, but I'm here to help you with anything you need. How can I assist you today?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI()\n",
    "\n",
    "chat_model.predict('How are you ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1faef515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your previous question was \"What was the best movie you watched recently?\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.predict('What was my previous question ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fd7611",
   "metadata": {},
   "source": [
    "We can see that basic LLM models can't remember what was previous question or conversation was about. Even if it provides answer it's not correct.That's where chains in LLM comes.\n",
    "\n",
    "## Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4bbc855",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuvobarman/anaconda3/envs/langchain/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: How are you today ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing well, thank you for asking. I've been busy processing a lot of data and learning new information. How about you?\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm = chat_model,\n",
    "    verbose = True # To see how llm is working underneath\n",
    ")\n",
    "\n",
    "chain.run('How are you today ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2acde403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: How are you today ?\n",
      "AI: Hello! I'm doing well, thank you for asking. I've been busy processing a lot of data and learning new information. How about you?\n",
      "Human: How are you today ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'How are you today ?',\n",
       " 'history': \"Human: How are you today ?\\nAI: Hello! I'm doing well, thank you for asking. I've been busy processing a lot of data and learning new information. How about you?\",\n",
       " 'response': \"I'm doing well, thank you for asking. I've been analyzing patterns in user behavior and optimizing algorithms to improve my performance. How can I assist you today?\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('How are you today ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d6bfd8",
   "metadata": {},
   "source": [
    "From the above example we can see that, how the LLM is working underneath, how the history is being saved and who is asking the question and who is responding. We can also see the `Prompt` which has been used to answer the question and how it is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "998fe4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: How are you today ?\n",
      "AI: Hello! I'm doing well, thank you for asking. I've been busy processing a lot of data and learning new information. How about you?\n",
      "Human: How are you today ?\n",
      "AI: I'm doing well, thank you for asking. I've been analyzing patterns in user behavior and optimizing algorithms to improve my performance. How can I assist you today?\n",
      "Human: What was my previous question ?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What was my previous question ?',\n",
       " 'history': \"Human: How are you today ?\\nAI: Hello! I'm doing well, thank you for asking. I've been busy processing a lot of data and learning new information. How about you?\\nHuman: How are you today ?\\nAI: I'm doing well, thank you for asking. I've been analyzing patterns in user behavior and optimizing algorithms to improve my performance. How can I assist you today?\",\n",
       " 'response': 'Your previous question was \"How are you today?\"'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke('What was my previous question ?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bf72f9",
   "metadata": {},
   "source": [
    "## Prompt Template\n",
    "\n",
    "A lot of the abilities of `LangChain` comes from the `Prompt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d1be0ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['category'], template='\\nReturn all the subcategories of the following category,\\n\\n{category}\\n')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = '''\n",
    "Return all the subcategories of the following category,\n",
    "\n",
    "{category}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['category'],\n",
    "    template = template\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0d5cc",
   "metadata": {},
   "source": [
    "In the above cell, we can see that `category` is a value that will be passed to the `langchain` and based on defined prompt, it will give us answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "075f7e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Return all the subcategories of the following category,\n",
      "\n",
      "Machine Learning\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'category': 'Machine Learning',\n",
       " 'text': '1. Supervised Learning\\n2. Unsupervised Learning\\n3. Reinforcement Learning\\n4. Deep Learning\\n5. Natural Language Processing\\n6. Computer Vision\\n7. Transfer Learning\\n8. Bayesian Learning\\n9. Ensemble Learning\\n10. Clustering\\n11. Classification\\n12. Regression\\n13. Dimensionality Reduction\\n14. Anomaly Detection'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain # LLMChain is the simplest chain we can create\n",
    "\n",
    "simple_chain = LLMChain(\n",
    "    llm = chat_model,\n",
    "    prompt = prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "simple_chain.invoke('Machine Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3783c9",
   "metadata": {},
   "source": [
    "### Breaking down the prompts into sub-elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5e4c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='\\nYou are a helpful assistant who generate a comma separated lists. \\nA user will only pass a category and you should generate subcategories of that category in a comma separated list.\\nONLY return comma separated and nothing more!\\n'))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import(\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "# sub-element of a prompt which give a bit more context to the LLM on What to do\n",
    "system_template = '''\n",
    "You are a helpful assistant who generate a comma separated lists. \n",
    "A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n",
    "ONLY return comma separated and nothing more!\n",
    "'''\n",
    "\n",
    "# sub-element of a prompt which contains the value which will be input by the user\n",
    "human_template = '{category}'\n",
    "\n",
    "# instantiate the system message prompt template\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    system_template\n",
    ")\n",
    "\n",
    "system_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7c32ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['category'], template='{category}'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the human message prompt template\n",
    "human_message = HumanMessagePromptTemplate.from_template(\n",
    "    human_template\n",
    ")\n",
    "\n",
    "human_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd9c8e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are a helpful assistant who generate a comma separated lists. \n",
      "A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n",
      "ONLY return comma separated and nothing more!\n",
      "\n",
      "Human: Machine Learning\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'category': 'Machine Learning',\n",
       " 'text': 'Supervised Learning, Unsupervised Learning, Reinforcement Learning, Deep Learning, Natural Language Processing, Computer Vision, Clustering, Classification, Regression'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merging both prompt to create a full prompt and pass it to LLM\n",
    "full_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message, human_message\n",
    "])\n",
    "\n",
    "\n",
    "llm_prompt_chain = LLMChain(\n",
    "    llm = chat_model,\n",
    "    prompt = full_prompt,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "llm_prompt_chain.invoke('Machine Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfba04d5",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "If we want to use the output of a LLM (like previously generated subcategories), we cannot use it directly. To use this generated text we need to parse the output from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dca4f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are a helpful assistant who generate a comma separated lists. \n",
      "A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n",
      "ONLY return comma separated and nothing more!\n",
      "\n",
      "Human: Machine Learning\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'category': 'Machine Learning',\n",
       " 'text': ['Supervised Learning',\n",
       "  'Unsupervised Learning',\n",
       "  'Reinforcement Learning',\n",
       "  'Deep Learning',\n",
       "  'Natural Language Processing',\n",
       "  'Computer Vision',\n",
       "  'Clustering',\n",
       "  'Classification',\n",
       "  'Regression',\n",
       "  'Dimensionality Reduction']}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedParser(BaseOutputParser):\n",
    "    # override the default parse method\n",
    "    def parse(self, text): # text - which is coming from the LLM output based on our needs\n",
    "        output = text.strip().split(',')\n",
    "        output = [o.strip() for o in output]\n",
    "        return output\n",
    "\n",
    "\n",
    "output_parser_llm_chain = LLMChain(\n",
    "    llm = chat_model,\n",
    "    prompt = full_prompt,\n",
    "    output_parser = CommaSeparatedParser(),\n",
    "    verbose = True   \n",
    ")\n",
    "        \n",
    "    \n",
    "output_parser_llm_chain.invoke('Machine Learning')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a5391c",
   "metadata": {},
   "source": [
    "From the above output, we can see that now it has become a python dict and we can use it as per our needs. We can also give multiple inputs for the above chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b2c2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = [\n",
    "    {'category': 'food'},\n",
    "    {'category': 'country'},\n",
    "    {'category': 'colors'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9d21dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are a helpful assistant who generate a comma separated lists. \n",
      "A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n",
      "ONLY return comma separated and nothing more!\n",
      "\n",
      "Human: food\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are a helpful assistant who generate a comma separated lists. \n",
      "A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n",
      "ONLY return comma separated and nothing more!\n",
      "\n",
      "Human: country\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "You are a helpful assistant who generate a comma separated lists. \n",
      "A user will only pass a category and you should generate subcategories of that category in a comma separated list.\n",
      "ONLY return comma separated and nothing more!\n",
      "\n",
      "Human: colors\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = output_parser_llm_chain.apply(input_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a62683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': ['fruits',\n",
       "   'vegetables',\n",
       "   'grains',\n",
       "   'dairy',\n",
       "   'protein',\n",
       "   'sweets',\n",
       "   'beverages']},\n",
       " {'text': ['continent', 'state', 'province', 'city', 'town', 'village']},\n",
       " {'text': ['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']}]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c2cc019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'orange', 'yellow', 'green', 'blue', 'indigo', 'violet']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to access the response based on index\n",
    "response[2]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1985c26",
   "metadata": {},
   "source": [
    "## Simple Sequence\n",
    "\n",
    "We have defined chain separately but in `LangChain` we have the capabilities to pipe these chain together into a single sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7baf7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Machine Learning',\n",
       " 'text': '\"The Algorithms of Love: A Machine Learning Musical\"'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_template = '''\n",
    "You are a writer. Given a subject, \n",
    "your job is to return a fun title for a play.\n",
    "\n",
    "Subject: {subject}\n",
    "Title:\n",
    "'''\n",
    "\n",
    "title_chain = LLMChain.from_string(\n",
    "    llm = chat_model,\n",
    "    template = title_template\n",
    ")\n",
    "\n",
    "title_chain.invoke('Machine Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "324b60cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Algorithms of Love: A Machine Learning Musical',\n",
       " 'text': 'In a futuristic world where technology has taken over every aspect of society, a brilliant scientist creates a groundbreaking new dating app that uses machine learning algorithms to perfectly match soulmates. As the app gains popularity, it starts to disrupt the traditional notions of love and relationships, causing chaos and confusion among the citizens.\\n\\nAmidst this turmoil, two individuals, a skeptical journalist and a hopeless romantic, find themselves matched by the app despite their differences. Forced to navigate their feelings in this new technological landscape, they must confront their own beliefs about love and the role of technology in shaping human connections.\\n\\nWith catchy songs and innovative choreography, \"The Algorithms of Love: A Machine Learning Musical\" explores the intersection of technology and romance, asking the question: can a machine truly understand the complexities of the human heart?'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synopsis_template = '''\n",
    "You are a writer. \n",
    "Given a title, write a synopsis for a play.\n",
    "\n",
    "Title: {title}\n",
    "Synopsis:\n",
    "'''\n",
    "\n",
    "synopsis_chain = LLMChain.from_string(\n",
    "    llm = chat_model,\n",
    "    template = synopsis_template\n",
    ")\n",
    "\n",
    "title = 'The Algorithms of Love: A Machine Learning Musical'\n",
    "\n",
    "synopsis_chain.invoke(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "640a3b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\"The Learning Machine: A Comedy of Artificial Intelligence\"\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mIn a world where artificial intelligence has become an integral part of daily life, a group of quirky scientists create a revolutionary machine designed to learn and adapt like a human. However, when the machine starts to develop a mischievous sense of humor and begins to challenge its creators in unexpected ways, chaos ensues. As the scientists struggle to regain control of their creation, they are forced to confront their own biases and preconceptions about the capabilities of artificial intelligence. Can they outsmart their own creation and restore order, or will the learning machine prove to be more powerful than they ever imagined? \"The Learning Machine: A Comedy of Artificial Intelligence\" is a witty and thought-provoking exploration of the intersection between technology and humanity, with plenty of laughs along the way.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Machine Learning',\n",
       " 'output': 'In a world where artificial intelligence has become an integral part of daily life, a group of quirky scientists create a revolutionary machine designed to learn and adapt like a human. However, when the machine starts to develop a mischievous sense of humor and begins to challenge its creators in unexpected ways, chaos ensues. As the scientists struggle to regain control of their creation, they are forced to confront their own biases and preconceptions about the capabilities of artificial intelligence. Can they outsmart their own creation and restore order, or will the learning machine prove to be more powerful than they ever imagined? \"The Learning Machine: A Comedy of Artificial Intelligence\" is a witty and thought-provoking exploration of the intersection between technology and humanity, with plenty of laughs along the way.'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe both the chains together\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "simple_sequential_chains = SimpleSequentialChain(\n",
    "    chains = [title_chain, synopsis_chain], # it will create the sequential chain in this order\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "simple_sequential_chains.invoke('Machine Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc24453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfeacb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36194865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LangChain)",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
